{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38790faa",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "https://wandb.ai/tcapelle/apple_m1_pro/reports/Deep-Learning-on-the-M1-Pro-with-Apple-Silicon---VmlldzoxMjQ0NjY3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ca096b",
   "metadata": {},
   "source": [
    "# I. Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594ecbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import string\n",
    "import random\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import wordnet # check misspelling\n",
    "from nltk.corpus import stopwords # check misspelling\n",
    "from wordcloud import WordCloud\n",
    "from spellchecker import SpellChecker # simple spell check correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbbc600",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load dataset\n",
    "df = pd.read_csv('######.csv',low_memory= False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4291a8",
   "metadata": {},
   "source": [
    "# II. Data First Look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6dcc1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955d8c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = df['DISC'].str.len().max()\n",
    "min_len = df['DISC'].str.len().min()\n",
    "\n",
    "print(\"Maximum length of string in dataset:\", max_len)\n",
    "print(\"Minimum length of string in dataset:\", min_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92462eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_string_lengths(df, col):\n",
    "    string_lengths = df[col].str.len()\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    ax.hist(string_lengths, bins=50, color='green', alpha=0.8, edgecolor='black')\n",
    "    ax.set_xlabel('Length of string', fontsize=12)\n",
    "    ax.set_ylabel('Frequency', fontsize=12)\n",
    "    ax.set_title('Distribution of String Lengths', fontsize=14)\n",
    "\n",
    "    ax.grid(axis='y', alpha=0.5)\n",
    "\n",
    "    # vertical line for the mean length\n",
    "    mean_length = string_lengths.mean()\n",
    "    ax.axvline(x=mean_length, color='red', linestyle='--', label=f'Mean length: {mean_length:.2f}')\n",
    "\n",
    "    # vertical line for the minimum length\n",
    "    min_length = string_lengths.min()\n",
    "    ax.axvline(x=min_length, color='blue', linestyle='--', label=f'Min length: {min_length}')\n",
    "\n",
    "    # vertical line for the maximum length\n",
    "    max_length = string_lengths.max()\n",
    "    ax.axvline(x=max_length, color='purple', linestyle='--', label=f'Max length: {max_length}')\n",
    "\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_string_lengths(df, 'DISC')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc05547",
   "metadata": {},
   "source": [
    "## Data Analysis:\n",
    "\n",
    "- The dataset consists of a single column of textual data.\n",
    "- There were no empty rows detected in the dataset.\n",
    "- The length of strings in the dataset ranged from 13 to 693 characters.\n",
    "- The majority of strings had a length of 180 characters, while the median length was 116.50 characters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b402bb8",
   "metadata": {},
   "source": [
    "# III. ETL Pipeline Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863a25c2",
   "metadata": {},
   "source": [
    "### Clean Dataset\n",
    "\n",
    "    - Make text all upper case\n",
    "    - Remove common non-sensical text (/n)\n",
    "    - Remove punctuations, leaving letters (A-Z) and digits (0-9) in the text\n",
    "    - Remove leading and trailing spaces\n",
    "    - Remove extra spaces (more than 1 whitespace characters) in the text\n",
    "    - Standardizing Words in the Dataset for Consistency After First Cleanning\n",
    "    - Remove customized stop words\n",
    "    - Tokenize text\n",
    "    - Remove customized stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4145c8dc",
   "metadata": {},
   "source": [
    "## 1. Make text uppercase, remove punctuations, and extra space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b81477e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    '''Make text uppercase, remove punctuations, remove punctuation and spaces'''\n",
    "    text = text.upper()\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub(r'[^a-zA-Z0-9]', ' ', text) # Remove all punctuations \n",
    "    # get rid of extra spaces after removing all punctuations in the previous step\n",
    "    text = re.sub('\\s+', ' ', text).strip()  \n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bbc4bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['clean'] = pd.DataFrame(df.DISC.apply(clean_text))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b578d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check the clean column by removing all letters and digits\n",
    "# df['punc'] = df['clean'].replace('[A-Z0-9]', '', regex=True)\n",
    "# df['punc'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98dccac7",
   "metadata": {},
   "source": [
    "## 2. Remove Customized Stopwords\n",
    "\n",
    "To avoid over-cleaning the already concise and short dataset, we opted not to remove all the stop words. Instead, we created a customized set of stop words to remove. This approach allows us to preserve the context and meaning of the text while removing unnecessary words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2188171b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "stop_words = ['A', 'AN', 'THE', 'THIS', 'THAT', 'THOSE', 'OR', 'EITHER',\n",
    "              'AND' ,'WAS', 'WERE', 'IS', 'ARE', 'BE', 'MAKE', 'MADE',\n",
    "              'AS', 'FOR','HAVE', 'HAS', 'HAD', 'DO', 'DID', 'DONE', 'LET',\n",
    "             'GET', 'GETS','GOT', 'GOTTEN', 'TAKE', 'TAKES', 'TOOK', 'TAKEN']\n",
    "# Tokenize the text data\n",
    "df['tokens'] = df['clean'].apply(lambda x: nltk.word_tokenize(x.upper()))\n",
    "\n",
    "# Filter out the stopwords\n",
    "df['clean'] = df['tokens'].apply(lambda x: ' '.join([word for word in x if word not in stop_words]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701618a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c08f01c",
   "metadata": {},
   "source": [
    "## 3. Standardizing Words in the Dataset for Consistency After First Cleanning\n",
    "In the dataset, some words are written in different formats, such as L.H, LH, L/H, and L//H. After removing the punctuations in the previous step, we notice that these words appear in two different forms: LH and L H. To ensure consistency in the data, we need to replace all instances of these words with a uniform format\n",
    "\n",
    "i.e. \n",
    "'LH' will be replace to 'L H' so that later in the IOB tagging step, the 'L' will be tagged as `B-loc`, and 'H' will be marked as `I-loc`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26a565e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to replace a specific word for later IOB tag, i.e.'LH' with 'LEFT HAND'\n",
    "def replace_word(word, old_word, new_word):\n",
    "    return word.replace(old_word, new_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0a35ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_replace = {\n",
    "    'LH': 'L H', # RIGHT HAND\n",
    "    'RH': 'R H', # LEFT HAND\n",
    "    'BS': 'B S', # BODY STATION\n",
    "    'I B': 'IB', # INBOUND\n",
    "    'O B': 'OB'} # OUTBOUND \n",
    "\n",
    "for old_word, new_word in words_to_replace.items():\n",
    "    df['clean'] = df['clean'].apply(lambda x: replace_word(x, old_word, new_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70cb34ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcaea6e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# random pick a row to check before and after cleaning\n",
    "idx = random.randint(2, len(df.index))\n",
    "print(df.loc[idx, 'DISC'])\n",
    "print(df.loc[idx, 'clean'])\n",
    "#MIR, T.O, REF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe582c10",
   "metadata": {},
   "source": [
    "## 4.a. Check Misspelling Words Using nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d42fbd",
   "metadata": {},
   "source": [
    "As we check for misspelled words in the text, we simultaneously record all `digits` we encounter in a set called digits. We will later use this set to identify the location of defects in the IOB tagging step. \n",
    "\n",
    "While there may be alternative ways to perform this check during the IOB tagging step, for now we will create the set and use it. We will refine the code at a later stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b25e974",
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = set() \n",
    "global_accept_words = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de72a2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(text):\n",
    "    global digits\n",
    "    # get the set of English stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    global global_accept_words\n",
    "    accept_words = []\n",
    "    misspelled_words = []\n",
    "    words = text.split()\n",
    "    for word in words:\n",
    "        if word.lower() in stop_words or word.isdigit():\n",
    "            accept_words.append(word)\n",
    "            if word.isdigit():\n",
    "                digits.add(word)\n",
    "            global_accept_words.append(word)\n",
    "            continue\n",
    "        for pos in ['n', 'v', 'a', 'r']: # noun, verb, adjective, adverb\n",
    "            if len(wordnet.synsets(word.lower(), pos=pos)) > 0:\n",
    "                # greater than 0 means the word is in WordNet\n",
    "                accept_words.append(word)\n",
    "        global_accept_words += accept_words\n",
    "        if word not in accept_words and not word.isdigit():\n",
    "            misspelled_words.append(word)\n",
    "    return accept_words, misspelled_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23385f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['accept_words'], df['misspelled_words'] = zip(*df['clean'].apply(process_text))\n",
    "df.loc[:, ['clean', 'accept_words', 'misspelled_words']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d85914a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "# convert the 'misspelled_words' column to a set\n",
    "typo_words = set(chain.from_iterable(df['misspelled_words'].values))\n",
    "\n",
    "# print(typo_words) # uncomment this line to see all possible typo words\n",
    "\n",
    "print(f\"Have found {len(typo_words)} acronyms and possible typos in the dataset\") # 3946"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09f9b25",
   "metadata": {},
   "source": [
    "### Have found 3978 acronyms and possible typos in the dataset\n",
    "\n",
    "As the dataset contains numerous acronyms commonly used in aircraft maintenance, it is difficult to determine the number of typos and their frequencies. To address this issue in the future, we plan to conduct research and create a list of all acronyms used in the dataset, which will enable us to identify misspelled words more accurately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380222da",
   "metadata": {},
   "source": [
    "## 4.b Check Misspelling Words Using OpenAi\n",
    "\n",
    "Create API key on OpenAi website, download and save it in your local machine. Make sure you have access to it later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9bd052c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import openai\n",
    "# import pandas as pd\n",
    "# import key\n",
    "\n",
    "# # Set up OpenAI API credentials\n",
    "# openai.api_key = key.key\n",
    "\n",
    "# def find_typos_gpt3(text):\n",
    "#     response = openai.Completion.create(\n",
    "#         engine=\"curie\", # davinci is expensive to run 'curie' > 'gpt2' > 'gptneo'\n",
    "#         prompt=f\"Find typos in the following text: '{text}'\",\n",
    "#         max_tokens=1024,\n",
    "#         n=1,\n",
    "#         stop=None,\n",
    "#         temperature=0.7,\n",
    "#     )\n",
    "#     return response.choices[0].text.strip()\n",
    "\n",
    "# df['typos'] = df['clean'].apply(find_typos_gpt3)\n",
    "\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6cd3e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(digits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f4c081",
   "metadata": {},
   "source": [
    "## 5.a Checking misspellings of 'CORROSION' in the text\n",
    "\n",
    "Although we won't be addressing misspelled words at the moment, we can quickly check the frequency of typos for the word 'CORROSION' in the dataset. This will provide us with an understanding of the noise level in the data and help determine the most appropriate approach to tackle it in the future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fe7035",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "misspelled_corrosion = dict()\n",
    "\n",
    "def find_misspelled_corrosion(report):\n",
    "    pattern = r\"COROSION|CORR|COROSSION\"\n",
    "    global global_accept_words\n",
    "    global misspelled_corrosion\n",
    "    global_accept_words.append('CORR') \n",
    "    words = report.split()\n",
    "    for word in words:\n",
    "        if re.search(pattern, word, re.IGNORECASE) and word not in global_accept_words:\n",
    "            if word not in misspelled_corrosion:     \n",
    "                misspelled_corrosion[word] = 0\n",
    "            misspelled_corrosion[word] += 1\n",
    "\n",
    "\n",
    "# misspelled_corrosion = df.clean.apply(find_misspelled_corrosion)\n",
    "df.clean.apply(find_misspelled_corrosion)\n",
    "print(misspelled_corrosion)\n",
    "print(f\"\\nHave found {len(misspelled_corrosion)} typos of the word 'CORROSION'\")\n",
    "print(f\"Those typos appear {sum(misspelled_corrosion.values())} times in the data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ac1560",
   "metadata": {},
   "source": [
    "### In the dataset, we have identified 91 misspellings of the word 'CORROSION', with a total frequency of 309 occurrences. Below is a list of all the misspelled words and their respective frequencies\n",
    "\n",
    "{'CORROSI': 10, 'CORROSSION': 25, 'COROSSION': 3, 'DEPOTCORROSION': 5, 'CORROD': 4, 'CORROED': 4, 'CORRION': 2, 'CORRISION': 23, 'CORRSION': 18, 'CORRO': 15, 'COROSION': 19, 'CORROISON': 4, 'CORREDED': 4, 'CORREDE': 2, 'CORROSIONALODINE': 1, 'CORRORDED': 1, 'CORROSIO': 14, 'ACCESSIBLECORROSION': 1, 'CORROSIONANNOTATE': 1, 'HCORROSIONB': 1, 'CORRDED': 2, 'CORRSOION': 3, 'CORROSOION': 2, 'CORRRECT': 1, 'CORROSIONN': 1, '643CORRODED': 2, 'CORROSIN': 7, 'CORROSON': 4, 'CORROS': 25, 'CORRROSION': 5, 'CELLCORRODED': 1, 'TANKCORROSION': 1, 'CORRIOSION': 5, 'CORROSIONDEFECT': 14, 'CORRREPAIR': 3, 'CORROSIONON': 6, 'CORROSIONINSTALL': 2, 'CORRBELOW': 1, 'HCORROSION': 3, 'CORRODRD': 1, 'CORROISION': 4, 'CORROSIONREMOVE': 2, 'CELLCORROSION': 1, 'CORROSIONAROUND': 1, 'CORRIOSIO': 1, 'CORROSIONCLEAN': 2, '90742CORROED': 1, 'CORROSIONAND': 3, 'CORRSOIN': 1, 'HCORRODED': 1, 'CORRECTIVEACTION': 2, 'HASCORR': 1, 'CORROSSIONREMOVED': 1, 'CORRISON': 3, 'CORROION': 1, 'SOMECORR': 1, '1902CORR': 1, '007190CORROSION': 1, 'CORRED': 1, 'CORRRION': 1, 'CORRECTI': 1, 'COROSIONIAW': 1, 'PDMCORROSION': 2, 'CORRODIDED': 1, '1925CORR': 1, 'WASHERSCORRODED': 1, 'CORROSOIN': 1, 'CORROTED': 1, 'CORROSED': 2, 'PDMCORRODED': 1, 'CORROSIONCOMPLETE': 1, 'ACFTCORRODED': 1, 'CORRODEDREQS': 1, 'CORRIONSION': 1, 'CORROSIOIN': 1, 'NOTCORROSION': 1, '229CORRODED': 1, 'ARECORR': 1, 'CORROEDED': 1, 'CORROSIION': 1, 'CORROSIONACCOMPLISH': 1, 'SEVERCORR': 1, 'CORROSIONFROM': 1, 'CORRIOSON': 1, 'CORROSIONIAW': 1, 'CORRODDED': 1, 'REMOVECORROSION': 1, 'ALLCORROSIONS': 1, 'CORROSIONINBETWEEN': 1, 'CORRE': 1, 'CORRIOSN': 1}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f223497e",
   "metadata": {},
   "source": [
    "## 5.b. Visualization of All Misspellings of the Word 'CORROSION' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279c6047",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "wordcloud = WordCloud(width=800, height=800, background_color='white').generate_from_frequencies(misspelled_corrosion)\n",
    "plt.figure(figsize=(6, 6), facecolor=None)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')\n",
    "plt.tight_layout(pad=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19079f58",
   "metadata": {},
   "source": [
    "## 6. Exploring the Effects of Basic Spell Correction on a Trial Dataframe\n",
    "\n",
    "In the context of my machine learning research report, I apply different spell correction methods to a trial dataframe that I have created and analyze the resulting outcomes. It's important to note that the trial dataframe used in this report was generated by the author and is not extracted from the original source data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eca0f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'text': ['FOUND CORRRODED BLDG 13', \n",
    "             'CHECK CRASHS IN LEF SIDE WING', \n",
    "             '2 ENG CORROSSION MARKED IN RED']}\n",
    "df_trial = pd.DataFrame(data)\n",
    "df_trial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14e4ba1",
   "metadata": {},
   "source": [
    "## 6.a. Spell Correction Trial with SpellChecker "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aeb37b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spellchecker import SpellChecker\n",
    "\n",
    "def correct_spelling(text):\n",
    "    \"\"\"\n",
    "    Takes in a text string and returns the corrected text using Spell Checker.\n",
    "    \"\"\"\n",
    "    spell = SpellChecker()\n",
    "    words = text.split()\n",
    "    corrected_words = []\n",
    "    for word in words:\n",
    "        corrected_word = spell.correction(word.lower())\n",
    "        if corrected_word is not None:\n",
    "            corrected_words.append(corrected_word.upper())\n",
    "        else:\n",
    "            corrected_words.append(word)\n",
    "    corrected_text = \" \".join(corrected_words)\n",
    "    return corrected_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9228528c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trial['correct_spell_checker'] = df_trial['text'].apply(correct_spelling)\n",
    "df_trial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b438c80",
   "metadata": {},
   "source": [
    "## 6.b. Spell Correction Trial with textblob\n",
    "\n",
    "TextBlob is a Python library that provides a simple API for natural language processing tasks, including spell checking. It uses a pre-trained model that is trained on a large corpus of text, including technical and industry-specific terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8024105",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "def correct_spelling_textblob(text):\n",
    "    \"\"\"\n",
    "    Takes in a text string and returns the corrected text using TextBlob.\n",
    "    \"\"\"\n",
    "    blob = TextBlob(text)\n",
    "    return str(blob.correct()).upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d275f6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trial['correct_textblob'] = df_trial['text'].apply(correct_spelling_textblob)\n",
    "df_trial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d9d6df",
   "metadata": {},
   "source": [
    "## 6.c. Spell Correction Trial with Gingerit\n",
    "\n",
    "GingerIt is a Python library that provides a simple API for spell checking and grammar correction. It uses a pre-trained model that is specifically designed for technical and industry-specific terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d37d3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gingerit.gingerit import GingerIt\n",
    "def correct_spelling_gingerit(text):\n",
    "    \"\"\"\n",
    "    Takes in a text string and returns the corrected text using GingerIt.\n",
    "    \"\"\"\n",
    "    parser = GingerIt()\n",
    "    result = parser.parse(text)\n",
    "    corrected_text = result['result']\n",
    "    return corrected_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15607594",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trial['correct_gingerit'] = df_trial['text'].apply(correct_spelling_gingerit)\n",
    "df_trial\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980dab9a",
   "metadata": {},
   "source": [
    "## 6.d. Spell Correction Trial with Language Tool\n",
    "\n",
    "LanguageTool is an open-source language checking tool that provides a REST API for spell checking and grammar correction. It supports more than 20 languages and can be used to check for grammar errors and spelling mistakes in technical and industry-specific texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbca79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import language_tool_python\n",
    "\n",
    "def correct_text_language_tool(text):\n",
    "    \"\"\"\n",
    "    Takes in a text string and returns the corrected text using Language Tool kit.\n",
    "    \"\"\"\n",
    "    my_tool = language_tool_python.LanguageTool('en-US')    \n",
    "    correct_text = my_tool.correct(text)  \n",
    "    return correct_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84bfc82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trial['correct_language_tool'] = df_trial['text'].apply(correct_text_language_tool)\n",
    "df_trial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d988993",
   "metadata": {},
   "source": [
    "## Limitations of ML Spelling Correction for Grammar Errors (Need Further Research)\n",
    "\n",
    "We have explored four distinct methods to correct the spelling in our dataset:\n",
    "\n",
    "- Approach 1 involved using a spell checker, which unfortunately resulted in the word 'BLDG' being incorrectly changed to 'BLOG'. However, it was able to identify and correct the misspelling of 'CORROSION' and 'CORRODED'.\n",
    "\n",
    "- Approach 2 utilized Textblob, which was unable to detect the misspelling of 'CORRRODED' and 'CORROSSION'. Additionally, it erroneously changed the word 'IN' to 'OF'. Although, it did not change the word 'BLDG' as the spell checker did.\n",
    "\n",
    "- In Approach 3, we employed Gingerit, which, like Textblob, did not identify the misspelling of 'CORRRODED' and 'CORROSSION'. However, it did not alter the word 'IN' or 'BLDG' in the original text.\n",
    "\n",
    "- Lastly, Approach 4 used Language Tool to successfully correct the misspelling of 'CORRODED', 'CORROSION', and 'CRASH'. Nevertheless, it removed the digit '2' from the original text.\n",
    "\n",
    "The tests have showed that none of the techniques we employed were able to identify the grammar error in the word 'LEF' or retain the digits from the original text. In general, we found that these methods were not successful in identifying grammar mistakes in our dataset, and sometimes even recommended wrong corrections. As a result, we have chosen to disregard them and plan to conduct further research or combine existing libraries to more effectively address typos\n",
    "\n",
    "Reference (future use):\n",
    "\n",
    "http://www.realworldnlpbook.com/blog/unreasonable-effectiveness-of-transformer-spell-checker.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15975413",
   "metadata": {},
   "source": [
    "## 7. Pickle the preprocessed dataframe into a pickle file fore later use (optional*)\n",
    "The preprocessed DataFrame is saved in a pickle file format called `preprocessed_data.pkl`. Later, when we need to use the preprocessed data, we can easily load it using the `pd.read_pickle` method\n",
    "\n",
    "(In the future, if we get a larger dataset, this step will facilitate faster loading of preprocessed data. This is because reading a pickle object is significantly faster than loading a CSV or Excel file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb09c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the preprocessed DataFrame in a pickle file format\n",
    "df.to_pickle(\"preprocessed_data.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7061c197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example to load the preprocessed DataFrame from the pickle file \n",
    "df = pd.read_pickle('preprocessed_data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512ce104",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ade5aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the required NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8c34f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e92f07",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "\n",
    "compound_corrosion = set()\n",
    "\n",
    "def find_adjacent_adjectives_and_adverbs(text):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    pos_tags = nltk.pos_tag(words)\n",
    "    adjectives_and_adverbs = []\n",
    "    \n",
    "    match_words = ('CORROSION', 'CORR', 'CORROSIONS', \n",
    "                   'CORROSIONS', 'CORROSIVE', 'CORRODING', 'CORRODED')\n",
    "    \n",
    "    for i, (word, pos) in enumerate(pos_tags):\n",
    "        if word in match_words:\n",
    "            compound_words = ''\n",
    "            for j in range(i-1, i-1-1, -1):\n",
    "#                 if pos_tags[j][1] not in ('IN', 'WB', 'WT', 'WP$', 'WRB'):  # except preposition or subordinating conjunction\n",
    "                  if pos_tags[j][1] in ('JJ', 'NN', 'VBD', 'RB'): \n",
    "                    compound_words = pos_tags[j][0] + ' ' + compound_words\n",
    "            if compound_words:\n",
    "                compound_words += word\n",
    "                compound_corrosion.add(compound_words)\n",
    "                adjectives_and_adverbs.append(compound_words)\n",
    "    return \" \".join(adjectives_and_adverbs)\n",
    "\n",
    "\n",
    "\n",
    "# apply the function to the \"clean\" column and create a new column with the results\n",
    "df['adjectives_and_corrosion'] = df['clean'].apply(find_adjacent_adjectives_and_adverbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c2d284",
   "metadata": {},
   "outputs": [],
   "source": [
    "cor = (set(df['adjectives_and_corrosion']))\n",
    "# Create a string from the set of words\n",
    "word_string = ' '.join(cor)\n",
    "\n",
    "# Create a WordCloud object and generate the word cloud\n",
    "wordcloud = WordCloud(background_color='white').generate(word_string)\n",
    "\n",
    "# Display the word cloud\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5e3328",
   "metadata": {},
   "outputs": [],
   "source": [
    "cor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd5d3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a sample set of words\n",
    "words = {'apple', 'banana', 'cherry', 'date', 'elderberry', 'fig', 'grape', 'honeydew', 'kiwi', 'lemon'}\n",
    "\n",
    "# Create a string from the set of words\n",
    "word_string = ' '.join(words)\n",
    "\n",
    "# Create a WordCloud object and generate the word cloud\n",
    "wordcloud = WordCloud(background_color='white').generate(word_string)\n",
    "\n",
    "# Display the word cloud\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0209323",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# open a file in binary mode to write the set into\n",
    "with open('compound_corrosions.pickle', 'wb') as f:\n",
    "    # use pickle.dump() to write the set into the file\n",
    "    pickle.dump(compound_corrosion, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5aeeba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the pickle file in binary mode for reading\n",
    "with open('compound_corrosions.pickle', 'rb') as f:\n",
    "    corrosion_set = pickle.load(f)\n",
    "\n",
    "# Print the set\n",
    "print(corrosion_set)\n",
    "print(f\"\\n have found {len(corrosion_set)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c123d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the function to the \"clean\" column and create a new column with the results\n",
    "df['adjectives_and_corrosion'] = df['clean'].apply(find_adjacent_adjectives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14a6d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681ca1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[47].clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fdd9e62",
   "metadata": {},
   "source": [
    "### Run the cell below to output the preprocessed dataframe to xlsx if you want to check the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc532fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp_df = pd.DataFrame({'clean_text': df['filtered_text']})\n",
    "# temp_df.to_excel('preprocessed_data.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcca02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Initialize a lemmatizer object\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Define a sample sentence containing a domain-specific term\n",
    "text = \"CORROSION TREAT APPLY\"\n",
    "\n",
    "# Split the sentence into individual words\n",
    "words = nltk.word_tokenize(text)\n",
    "\n",
    "# Lemmatize the words\n",
    "lemmatized_words = [lemmatizer.lemmatize(word.lower()) for word in words]\n",
    "\n",
    "# Join the lemmatized words back into a sentence\n",
    "lemmatized_text = ' '.join(lemmatized_words)\n",
    "\n",
    "# Print the original and lemmatized sentences\n",
    "print(\"Original text:\", text)\n",
    "print(\"Lemmatized text:\", lemmatized_text.upper())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1b810f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
